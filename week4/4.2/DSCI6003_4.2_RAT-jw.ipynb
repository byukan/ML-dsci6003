{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAT 4.2\n",
    "\n",
    "1 - \n",
    "\n",
    "A recap of regularization: Describe weaknesses and strengths of Lasso, Ridge and Elstic Net Regularization\n",
    "\n",
    "2 - \n",
    "\n",
    "Discuss in plain English the Regularization Paths (\"Squid\" plots) in terms of each of the above three regularization techniques. For reference, here is what a regularization path looks like: \n",
    "\n",
    "![Squid Plots](http://scikit-learn.sourceforge.net/0.6/_images/plot_lasso_coordinate_descent_path.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://www.mathworks.com/help/stats/lasso-regularization-of-generalized-linear-models.html?requestedDomain=www.mathworks.com\n",
    "\n",
    "https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html\n",
    "\n",
    "minβ0,β1N∑i=1Nwil(yi,β0+βTxi)+λ[(1−α)||β||22/2+α||β||1],\n",
    "\n",
    "weaknesses and strengths of Lasso, Ridge and Elastic Net Regularization\n",
    "\n",
    "lasso removes less important features by producing 0 coefficients, and uses the sum of the absolute values of the beta coefficients (L1 norm of beta).\n",
    "pros of lasso are that it creates a smaller model, \n",
    "\n",
    "\n",
    "elastic net combines lasso and ridge.  Empirical studies suggest that the elastic net technique can outperform lasso on data with highly correlated predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "in plain english, the regularization paths/squid plots show the value of the beta coefficients as we relax the penalization term.  We can see from the example plot that with a high regularization term, lambda, (the lhs of the graph), the coefficients are close to 0, and with low values, the lasso coefficients pop up, because lasso chooses the winners (as penalization relaxes, new coefficients start to pop up).\n",
    "\n",
    "With ridge regression, which is not in the picture, the less important features would start of with super small weight, but all would be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
