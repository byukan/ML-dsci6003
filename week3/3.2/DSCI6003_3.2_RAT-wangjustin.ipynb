{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAT 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "1. How does best subset selection approach regularization of OLS?\n",
    "2. How is ridge regression similar to best subset selection?\n",
    "3. Outline the steps for finding variable importance with linear regression\n",
    "4. K-fold cross-validation yields K trained models.  Which one do you deploy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#1. best subset selection considers the best possible subset iteratively, starting at 1, and going up to the full size of the feature set of all attributes.  The quality of the model is judged by how well it models the sample data.  It is very impractical to compute because we consider all subsets of size 1 and select the best, then select all subsets of size 2, and so on.\n",
    "\n",
    "#2. Best subset selection incorporates a series of constraints on the number of attributes in the model.  Similarly, ridge regression is a penalty method that constrains the model; however, instead of contraining the number of attribues, ridge regression constrains the total amount of coefficient that can be used across all the attributes.  It's ok to use every attribute, so long as the amount of each is small.\n",
    "\n",
    "#3. ridge regression is a penalized linear regression method.  Let $y_i$ be the labels of the attributes, and let $x_i$ be the labels of the row attribues.  Set up a linear model releating the attribues to the rows by constructin a weight vector w equal in size to the number of attribues, and include a bias b.  \n",
    "\n",
    "Then the linear regression problem is to find the weight vector w∗w∗ and the bias b∗b∗ satisfying\n",
    "\n",
    "w∗,b∗=argminw,b1M∑Mi=1(yi−(b+xiwT))2w∗,b∗=argminw,b1M∑i=1M(yi−(b+xiwT))2\n",
    "\n",
    "The best values for the weights and bias (w∗andb∗w∗andb∗) are the ones that minimize the mean squared error. The ridge regression formulation adds a weight penalty to this formulation. For ridge regression the best values satisfy\n",
    "\n",
    "w∗,b∗=argminw,b1M∑Mi=1(yi−(b+xiwT))2+$\\alpha$wwT\n",
    "\n",
    "So, $\\alpha$wwT is the weight penalty in ridge regression.\n",
    "\n",
    "#4. The k results from the folds can be averaged to produce a single estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
