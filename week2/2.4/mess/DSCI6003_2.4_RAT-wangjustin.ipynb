{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAT 2.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Summarize the Curse of Dimensionality and its use to explain poor performance of locality-based methods in high dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio of the area of a box to the area of a circle inscribed inside is less than the ratio of the volume of a cube to the volume of an inscribed sphere.  The ratio is 4*pi: 6*pi.  The ratio continues to increase as the dimmension increases.  As the dimmension increases, points become further and further apart, which makes it difficult to identify information that could be significant, but are close together.\n",
    "\n",
    "The curse of dimensionality is a phrase that refers to a group of observational phenomena that occur when dealing with data of dimensionality greater than 3, i.e. vectors/matrices/tensors of length greater than three. It should be obvious that most real world data fits this description if we are to consider it in matrix format.\n",
    "\n",
    "As the number of dimensions increases, the [metric space](https://en.wikipedia.org/wiki/Metric_space) or distance relationships between points of data in the data set, becomes deformed. This leads to the following phenomena:\n",
    "\n",
    "1. Points separate into extrema. Points become either extremely close together or extremely far apart. Nearest points are often extremely far away, so that any differences between the distances tend towards being smaller.\n",
    "2. Geometric intuition breaks down. As a consequence of (1.), geometric intuition about algorithms breaks down. It becomes more difficult to interpret comparative values of cost function and thus harder to understand why the algorithm fails.\n",
    "3. The computational cost of managing long data vectors increases relative to their length. (This can become a significant performance penalty when algorithms need to loop over the length of the vector.)\n",
    "4. Exponential sampling increase. As d gets large, the number of vectors that are almost orthogonal increases rapidly. This causes sampling to become inefficient, as the number of data points required to adequately describe the sample space increases exponentially!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - Suppose N = 100 represents a dense sample for a three dimensional feature space.\n",
    "To achieve same density in an eight dimensional feature space, how many points would we need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10000000 = 10M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
